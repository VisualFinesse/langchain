{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "id": "flgjSe_tv6pR"
      },
      "source": [
        "---\n",
        "title: Code understanding\n",
        "sidebar_class_name: hidden\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pP39DWaFv6pT"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/use_cases/code_understanding.ipynb)\n",
        "\n",
        "## Use case\n",
        "\n",
        "Source code analysis is one of the most popular LLM applications (e.g., [GitHub Copilot](https://github.com/features/copilot), [Code Interpreter](https://chat.openai.com/auth/login?next=%2F%3Fmodel%3Dgpt-4-code-interpreter), [Codium](https://www.codium.ai/), and [Codeium](https://codeium.com/about)) for use-cases such as:\n",
        "\n",
        "- Q&A over the code base to understand how it works\n",
        "- Using LLMs for suggesting refactors or improvements\n",
        "- Using LLMs for documenting the code\n",
        "\n",
        "![Image description](https://github.com/langchain-ai/langchain/blob/master/docs/static/img/code_understanding.png?raw=1)\n",
        "\n",
        "## Overview\n",
        "\n",
        "The pipeline for QA over code follows [the steps we do for document question answering](/docs/use_cases/question_answering), with some differences:\n",
        "\n",
        "In particular, we can employ a [splitting strategy](/docs/integrations/document_loaders/source_code) that does a few things:\n",
        "\n",
        "* Keeps each top-level function and class in the code is loaded into separate documents.\n",
        "* Puts remaining into a separate document.\n",
        "* Retains metadata about where each split comes from\n",
        "\n",
        "## Quickstart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dsCwhUQhv6pW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Repository Path: C:\\\\Users\\\\mfoster\\\\source\\\\repos\\\\Renovo\n"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade --quiet langchain-openai pymilvus tiktoken langchain-chroma langchain GitPython\n",
        "\n",
        "# Import necessary library to handle .env files\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load the environment variables from .env\n",
        "load_dotenv()\n",
        "\n",
        "# Now you can use os.getenv to access your environment variables\n",
        "repository_path = os.getenv('REPOSITORY_PATH')\n",
        "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "# Print the variables to confirm they are loaded correctly\n",
        "print(\"Repository Path:\", repository_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pymilvus import connections\n",
        "connections.connect(\"default\", host=\"localhost\", port=\"19530\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFcl4pc1v6pX"
      },
      "source": [
        "We'll follow the structure of [this notebook](https://github.com/cristobalcl/LearningLangChain/blob/master/notebooks/04%20-%20QA%20with%20code.ipynb) and employ [context aware code splitting](/docs/integrations/document_loaders/source_code)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxoWEKVEv6pY"
      },
      "source": [
        "### Loading\n",
        "\n",
        "\n",
        "We will upload all python project files using the `langchain_community.document_loaders.TextLoader`.\n",
        "\n",
        "The following script iterates over the files in the LangChain repository and loads every `.py` file (a.k.a. **documents**):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "CZgYS8N_v6pY"
      },
      "outputs": [],
      "source": [
        "from git import Repo\n",
        "from langchain_community.document_loaders.generic import GenericLoader\n",
        "from langchain_community.document_loaders.parsers import LanguageParser\n",
        "from langchain_text_splitters import Language"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "De5AiZTFv6pZ"
      },
      "source": [
        "We load the py code using [`LanguageParser`](/docs/integrations/document_loaders/source_code), which will:\n",
        "\n",
        "* Keep top-level functions and classes together (into a single document)\n",
        "* Put remaining code into a separate document\n",
        "* Retains metadata about where each split comes from"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Tgfv5Sb1v6pZ",
        "outputId": "747bfe7f-3a7c-48cd-a9ad-cce1c9c0e9ad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load\n",
        "loader = GenericLoader.from_filesystem(\n",
        "    repository_path + \"/libs/core/langchain_core\",\n",
        "    glob=\"**/*\",\n",
        "    suffixes=[\".py\"],\n",
        "    exclude=[\"**/non-utf8-encoding.py\"],\n",
        "    parser=LanguageParser(language=Language.PYTHON, parser_threshold=500),\n",
        ")\n",
        "documents = loader.load()\n",
        "len(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0wvvTLMv6pa"
      },
      "source": [
        "### Splitting\n",
        "\n",
        "Split the `Document` into chunks for embedding and vector storage.\n",
        "\n",
        "We can use `RecursiveCharacterTextSplitter` w/ `language` specified."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "piG2niJov6pa",
        "outputId": "02059c5c-2cf8-4317-c2d1-69976f542291"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
        "    language=Language.PYTHON, chunk_size=2000, chunk_overlap=200\n",
        ")\n",
        "texts = python_splitter.split_documents(documents)\n",
        "len(texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqu1-ad3v6pb"
      },
      "source": [
        "### RetrievalQA\n",
        "\n",
        "We need to store the documents in a way we can semantically search for their content.\n",
        "\n",
        "The most common approach is to embed the contents of each document then store the embedding and document in a vector store.\n",
        "\n",
        "When setting up the vectorstore retriever:\n",
        "\n",
        "* We test [max marginal relevance](/docs/use_cases/question_answering) for retrieval\n",
        "* And 8 documents returned\n",
        "\n",
        "#### Go deeper\n",
        "\n",
        "- Browse the > 40 vectorstores integrations [here](https://integrations.langchain.com/).\n",
        "- See further documentation on vectorstores [here](/docs/modules/data_connection/vectorstores/).\n",
        "- Browse the > 30 text embedding integrations [here](https://integrations.langchain.com/).\n",
        "- See further documentation on embedding models [here](/docs/modules/data_connection/text_embedding/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "rBM9kTg-v6pb"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Expected IDs to be a non-empty list, got []",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[20], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_chroma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Chroma\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbeddings\n\u001b[1;32m----> 4\u001b[0m db \u001b[38;5;241m=\u001b[39m \u001b[43mChroma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOpenAIEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdisallowed_special\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m retriever \u001b[38;5;241m=\u001b[39m db\u001b[38;5;241m.\u001b[39mas_retriever(\n\u001b[0;32m      6\u001b[0m     search_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmmr\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Also test \"similarity\"\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     search_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m8\u001b[39m},\n\u001b[0;32m      8\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\mfoster\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_chroma\\vectorstores.py:791\u001b[0m, in \u001b[0;36mChroma.from_documents\u001b[1;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[0;32m    789\u001b[0m texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m    790\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m--> 791\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\mfoster\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_chroma\\vectorstores.py:749\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[0;32m    741\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mchromadb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_batches\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m create_batches(\n\u001b[0;32m    744\u001b[0m         api\u001b[38;5;241m=\u001b[39mchroma_collection\u001b[38;5;241m.\u001b[39m_client,\n\u001b[0;32m    745\u001b[0m         ids\u001b[38;5;241m=\u001b[39mids,\n\u001b[0;32m    746\u001b[0m         metadatas\u001b[38;5;241m=\u001b[39mmetadatas,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    747\u001b[0m         documents\u001b[38;5;241m=\u001b[39mtexts,\n\u001b[0;32m    748\u001b[0m     ):\n\u001b[1;32m--> 749\u001b[0m         \u001b[43mchroma_collection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[0;32m    752\u001b[0m \u001b[43m            \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    753\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    754\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    755\u001b[0m     chroma_collection\u001b[38;5;241m.\u001b[39madd_texts(texts\u001b[38;5;241m=\u001b[39mtexts, metadatas\u001b[38;5;241m=\u001b[39mmetadatas, ids\u001b[38;5;241m=\u001b[39mids)\n",
            "File \u001b[1;32mc:\\Users\\mfoster\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_chroma\\vectorstores.py:355\u001b[0m, in \u001b[0;36mChroma.add_texts\u001b[1;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m    349\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collection\u001b[38;5;241m.\u001b[39mupsert(\n\u001b[0;32m    350\u001b[0m             embeddings\u001b[38;5;241m=\u001b[39membeddings_without_metadatas,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    351\u001b[0m             documents\u001b[38;5;241m=\u001b[39mtexts_without_metadatas,\n\u001b[0;32m    352\u001b[0m             ids\u001b[38;5;241m=\u001b[39mids_without_metadatas,\n\u001b[0;32m    353\u001b[0m         )\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 355\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_collection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupsert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m        \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[0;32m    357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m        \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
            "File \u001b[1;32mc:\\Users\\mfoster\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\chromadb\\api\\models\\Collection.py:477\u001b[0m, in \u001b[0;36mCollection.upsert\u001b[1;34m(self, ids, embeddings, metadatas, documents, images, uris)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupsert\u001b[39m(\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    446\u001b[0m     ids: OneOrMany[ID],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    456\u001b[0m     uris: Optional[OneOrMany[URI]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    457\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    458\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Update the embeddings, metadatas or documents for provided ids, or create them if they don't exist.\u001b[39;00m\n\u001b[0;32m    459\u001b[0m \n\u001b[0;32m    460\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;124;03m        None\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    470\u001b[0m     (\n\u001b[0;32m    471\u001b[0m         ids,\n\u001b[0;32m    472\u001b[0m         embeddings,\n\u001b[0;32m    473\u001b[0m         metadatas,\n\u001b[0;32m    474\u001b[0m         documents,\n\u001b[0;32m    475\u001b[0m         images,\n\u001b[0;32m    476\u001b[0m         uris,\n\u001b[1;32m--> 477\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_embedding_set\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muris\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    481\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m embeddings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m documents \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\mfoster\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\chromadb\\api\\models\\Collection.py:545\u001b[0m, in \u001b[0;36mCollection._validate_embedding_set\u001b[1;34m(self, ids, embeddings, metadatas, documents, images, uris, require_embeddings_or_data)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_embedding_set\u001b[39m(\n\u001b[0;32m    524\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    525\u001b[0m     ids: OneOrMany[ID],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    543\u001b[0m     Optional[URIs],\n\u001b[0;32m    544\u001b[0m ]:\n\u001b[1;32m--> 545\u001b[0m     valid_ids \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_cast_one_to_many_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    546\u001b[0m     valid_embeddings \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    547\u001b[0m         validate_embeddings(\n\u001b[0;32m    548\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_normalize_embeddings(maybe_cast_one_to_many_embedding(embeddings))\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    551\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    552\u001b[0m     )\n\u001b[0;32m    553\u001b[0m     valid_metadatas \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    554\u001b[0m         validate_metadatas(maybe_cast_one_to_many_metadata(metadatas))\n\u001b[0;32m    555\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m metadatas \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    556\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    557\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\mfoster\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\chromadb\\api\\types.py:228\u001b[0m, in \u001b[0;36mvalidate_ids\u001b[1;34m(ids)\u001b[0m\n\u001b[0;32m    226\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected IDs to be a list, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mids\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ids) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected IDs to be a non-empty list, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mids\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    229\u001b[0m seen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m    230\u001b[0m dups \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[1;31mValueError\u001b[0m: Expected IDs to be a non-empty list, got []"
          ]
        }
      ],
      "source": [
        "from langchain_chroma import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "db = Chroma.from_documents(texts, OpenAIEmbeddings(disallowed_special=()))\n",
        "retriever = db.as_retriever(\n",
        "    search_type=\"mmr\",  # Also test \"similarity\"\n",
        "    search_kwargs={\"k\": 8},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPqD3iA7v6pb"
      },
      "source": [
        "### Chat\n",
        "\n",
        "Test chat, just as we do for [chatbots](/docs/use_cases/chatbots).\n",
        "\n",
        "#### Go deeper\n",
        "\n",
        "- Browse the > 55 LLM and chat model integrations [here](https://integrations.langchain.com/).\n",
        "- See further documentation on LLMs and chat models [here](/docs/modules/model_io/).\n",
        "- Use local LLMS: The popularity of [PrivateGPT](https://github.com/imartinez/privateGPT) and [GPT4All](https://github.com/nomic-ai/gpt4all) underscore the importance of running LLMs locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WK_0lMJCv6pb"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4\")\n",
        "\n",
        "# First we need a prompt that we can pass into an LLM to generate this search query\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"placeholder\", \"{chat_history}\"),\n",
        "        (\"user\", \"{input}\"),\n",
        "        (\n",
        "            \"user\",\n",
        "            \"Given the above conversation, generate a search query to look up to get information relevant to the conversation\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "retriever_chain = create_history_aware_retriever(llm, retriever, prompt)\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"Answer the user's questions based on the below context:\\n\\n{context}\",\n",
        "        ),\n",
        "        (\"placeholder\", \"{chat_history}\"),\n",
        "        (\"user\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "document_chain = create_stuff_documents_chain(llm, prompt)\n",
        "\n",
        "qa = create_retrieval_chain(retriever_chain, document_chain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQIvvK1Bv6pb",
        "outputId": "3c12a9d4-7fa9-48e6-9ba0-92e4fcbeba32"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'A RunnableBinding is a class in the LangChain library that is used to bind arguments to a Runnable. This is useful when a runnable in a chain requires an argument that is not in the output of the previous runnable or included in the user input. It returns a new Runnable with the bound arguments and configuration. The bind method in the RunnableBinding class is used to perform this operation.'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question = \"What is a RunnableBinding?\"\n",
        "result = qa.invoke({\"input\": question})\n",
        "result[\"answer\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1HQb5rYv6pb",
        "outputId": "286687c5-afbe-44d6-97b2-98fdc7c5cca7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-> **Question**: What classes are derived from the Runnable class? \n",
            "\n",
            "**Answer**: The classes derived from the `Runnable` class as mentioned in the context are: `RunnableLambda`, `RunnableLearnable`, `RunnableSerializable`, `RunnableWithFallbacks`. \n",
            "\n",
            "-> **Question**: What one improvement do you propose in code in relation to the class hierarchy for the Runnable class? \n",
            "\n",
            "**Answer**: One potential improvement could be the introduction of abstract base classes (ABCs) or interfaces for different types of Runnable classes. Currently, it seems like there are lots of different Runnable types, like RunnableLambda, RunnableParallel, etc., each with their own methods and attributes. By defining a common interface or ABC for all these classes, we can ensure consistency and better organize the codebase. It would also make it easier to add new types of Runnable classes in the future, as they would just need to implement the methods defined in the interface or ABC. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "questions = [\n",
        "    \"What classes are derived from the Runnable class?\",\n",
        "    \"What one improvement do you propose in code in relation to the class hierarchy for the Runnable class?\",\n",
        "]\n",
        "\n",
        "for question in questions:\n",
        "    result = qa.invoke({\"input\": question})\n",
        "    print(f\"-> **Question**: {question} \\n\")\n",
        "    print(f\"**Answer**: {result['answer']} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GFdfg1Jv6pc"
      },
      "source": [
        "Then we can look at the [LangSmith trace](https://smith.langchain.com/public/616f6620-f49f-46c7-8f4b-dae847705c5d/r) to see what is happening under the hood:\n",
        "\n",
        "* In particular, the code well structured and kept together in the retrieval output\n",
        "* The retrieved code and chat history are passed to the LLM for answer distillation\n",
        "\n",
        "![Image description](https://github.com/langchain-ai/langchain/blob/master/docs/static/img/code_retrieval.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbnXgcEOv6pc"
      },
      "source": [
        "### Open source LLMs\n",
        "\n",
        "We'll use LangChain's [Ollama integration](https://ollama.com/) to query a local OSS model.\n",
        "\n",
        "Check out the latest available models [here](https://ollama.com/library)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Cs17ykXv6pc"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hMZI1Mmv6pc"
      },
      "outputs": [],
      "source": [
        "from langchain_community.chat_models.ollama import ChatOllama\n",
        "\n",
        "llm = ChatOllama(model=\"codellama\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6hqb9tnv6pc"
      },
      "source": [
        "Let's run it with a generic coding question to test its knowledge:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwMWl__cv6pc",
        "outputId": "3e0a8de5-3c7d-4422-8bb7-ae6f24ce603b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "You can use the `find` command with the `-mtime` option to find all the text files in the current directory that have been modified in the last month. Here's an example command:\n",
            "```bash\n",
            "find . -type f -name \"*.txt\" -mtime -30\n",
            "```\n",
            "This will list all the text files in the current directory (`.`) that have been modified in the last 30 days. The `-type f` option ensures that only regular files are matched, and not directories or other types of files. The `-name \"*.txt\"` option restricts the search to files with a `.txt` extension. Finally, the `-mtime -30` option specifies that we want to find files that have been modified in the last 30 days.\n",
            "\n",
            "You can also use `find` command with `-mmin` option to find all the text files in the current directory that have been modified within the last month. Here's an example command:\n",
            "```bash\n",
            "find . -type f -name \"*.txt\" -mmin -4320\n",
            "```\n",
            "This will list all the text files in the current directory (`.`) that have been modified within the last 30 days. The `-type f` option ensures that only regular files are matched, and not directories or other types of files. The `-name \"*.txt\"` option restricts the search to files with a `.txt` extension. Finally, the `-mmin -4320` option specifies that we want to find files that have been modified within the last 4320 minutes (which is equivalent to one month).\n",
            "\n",
            "You can also use `ls` command with `-l` option and pipe it to `grep` command to filter out the text files. Here's an example command:\n",
            "```bash\n",
            "ls -l | grep \"*.txt\"\n",
            "```\n",
            "This will list all the text files in the current directory (`.`) that have been modified within the last 30 days. The `-l` option of `ls` command lists the files in a long format, including the modification time, and the `grep` command filters out the files that do not match the specified pattern.\n",
            "\n",
            "Please note that these commands are case-sensitive, so if you have any files with different extensions (e.g., `.TXT`), they will not be matched by these commands.\n",
            "{'model': 'codellama', 'created_at': '2024-04-03T00:41:44.014203Z', 'message': {'role': 'assistant', 'content': ''}, 'done': True, 'total_duration': 27078466916, 'load_duration': 12947208, 'prompt_eval_count': 44, 'prompt_eval_duration': 11497468000, 'eval_count': 510, 'eval_duration': 15548191000}\n"
          ]
        }
      ],
      "source": [
        "response_message = llm.invoke(\n",
        "    \"In bash, how do I list all the text files in the current directory that have been modified in the last month?\"\n",
        ")\n",
        "\n",
        "print(response_message.content)\n",
        "print(response_message.response_metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gY_-Nu88v6pc"
      },
      "source": [
        "Looks reasonable! Now let's set it up with our previously loaded vectorstore.\n",
        "\n",
        "We omit the conversational aspect to keep things more manageable for the lower-powered local model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AVX8sZNv6pc"
      },
      "outputs": [],
      "source": [
        "# from langchain.chains.question_answering import load_qa_chain\n",
        "\n",
        "# # Prompt\n",
        "# template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
        "# If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "# Use three sentences maximum and keep the answer as concise as possible.\n",
        "# {context}\n",
        "# Question: {question}\n",
        "# Helpful Answer:\"\"\"\n",
        "# QA_CHAIN_PROMPT = PromptTemplate(\n",
        "#     input_variables=[\"context\", \"question\"],\n",
        "#     template=template,\n",
        "# )\n",
        "\n",
        "system_template = \"\"\"\n",
        "Answer the user's questions based on the below context.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "Use three sentences maximum and keep the answer as concise as possible:\n",
        "\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "# First we need a prompt that we can pass into an LLM to generate this search query\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_template),\n",
        "        (\"user\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "document_chain = create_stuff_documents_chain(llm, prompt)\n",
        "\n",
        "qa_chain = create_retrieval_chain(retriever, document_chain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KimMOZcwv6pc",
        "outputId": "65df40b5-3d6f-401a-8e4e-0e7b7e73a0fa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"A RunnableBinding is a high-level class in the LangChain framework. It's an abstraction layer that sits between a program and an LLM or other data source.\\n\\nThe main goal of a RunnableBinding is to enable a program, which may be a chat bot or a backend service, to fetch responses from an LLM or other data sources in a way that is easy for both the program and the data sources to use. This is achieved through a set of predefined protocols that are implemented by the RunnableBinding.\\n\\nThe protocols defined by a RunnableBinding include:\\n\\n1. Fetching inputs from the program. The RunnableBinding should be able to receive inputs from the program and translate them into a format that can be processed by the LLM or other data sources.\\n2. Translating outputs from the LLM or other data sources into something that can be returned to the program. This includes converting the raw output of an LLM into something that is easier for the program to process, such as text or a structured object.\\n3. Handling errors that may arise during the fetching, processing, and returning of responses from the LLM or other data sources. The RunnableBinding should be able to catch exceptions and errors that occur during these operations and return a suitable error message or response to the program.\\n4. Managing concurrency and parallelism in the communication with the LLM or other data sources. This may include things like allowing multiple requests to be sent to the LLM or other data sources simultaneously, handling the responses asynchronously, and retrying failed requests.\\n5. Providing a way for the program to set configuration options that affect how the RunnableBinding interacts with the LLM or other data sources. This could include things like setting up credentials, providing additional contextual information to the LLM or other data sources, and controlling logging or error handling behavior.\\n\\nIn summary, a RunnableBinding provides a way for a program to easily communicate with an LLM or other data sources without having to know about the details of how they work. By providing a consistent interface between the program and the data sources, the RunnableBinding enables more robust and scalable communication protocols that are easier for both parties to use.\\n\\nIn the context of the chatbot tutorial, a RunnableBinding may be used to fetch responses from an LLM and return them as output for the bot to process. The RunnableBinding could also be used to handle errors that occur during this process, such as providing error messages or retrying failed requests to the LLM.\\n\\nTo summarize:\\n\\n* A RunnableBinding provides a way for a program to communicate with an LLM or other data sources without having to know about the details of how they work.\\n* It enables more robust and scalable communication protocols that are easier for both parties to use.\\n* It manages concurrency and parallelism in the communication with the LLM or other data sources.\\n* It provides a way for the program to set configuration options that affect how the RunnableBinding interacts with the LLM or other data sources.\""
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Run, only returning the value under the answer key for readability\n",
        "qa_chain.pick(\"answer\").invoke({\"input\": \"What is a RunnableBinding?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prMMrGRFv6pc"
      },
      "source": [
        "Not perfect, but it did pick up on the fact that it lets the developer set configuration option!\n",
        "\n",
        "Here's the [LangSmith trace](https://smith.langchain.com/public/d8bb2af8-99cd-406b-a870-f255f4a2423c/r) showing the retrieved docs used as context."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
