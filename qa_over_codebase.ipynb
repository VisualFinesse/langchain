{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install necessary packages\n",
    "%pip install --upgrade --quiet langchain-openai tiktoken langchain-chroma langchain GitPython pymilvus langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository Path: C:\\Users\\mfoster\\source\\repos\\langchain\n",
      "KnowledgeBase Path: C:\\Users\\mfoster\\source\\repos\\kb_test\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "repository_path = os.getenv('REPOSITORY_PATH')\n",
    "knowledgebase_dir = os.getenv('KNOWLEDGEBASE_DIR')\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "print(\"Repository Path:\", repository_path)\n",
    "print(\"KnowledgeBase Path:\", knowledgebase_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import GitPython for repository operations\n",
    "from git import Repo\n",
    "\n",
    "# Document loading and parsing\n",
    "from langchain_community.document_loaders.generic import GenericLoader\n",
    "from langchain_community.document_loaders.parsers import LanguageParser\n",
    "from langchain_text_splitters import Language\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for ignore_list.txt at: C:\\Users\\mfoster\\source\\repos\\langchain\\ignore_list.txt\n",
      "Ignore List: ['**/node_modules/**', '.git', '.env', 'build', 'temp', '*.log', '*__pycache__', '**/non-utf8-encoding.py']\n",
      "Directory to load files from: C:\\Users\\mfoster\\source\\repos\\kb_test\\libs/core/langchain_core\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 234.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents Loaded: 2\n",
      "\u001b[92mtestdoc.txt\u001b[0m\n",
      "\u001b[92mkb_subfolder\\subfile.txt\u001b[0m\n",
      "Ignored: \u001b[91mkb_subfolder\\node_modules\\node_moduletest.txt\u001b[0m\n",
      "Ignored: \u001b[91mnode_modules\\nodemodulefile.txt\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the ignore list from ignore_list.txt in the root project directory\n",
    "ignore_list_path = os.path.join(repository_path, 'ignore_list.txt')\n",
    "print(\"Looking for ignore_list.txt at:\", ignore_list_path)\n",
    "try:\n",
    "    with open(ignore_list_path, 'r') as file:\n",
    "        ignore_list = [line.strip() for line in file.readlines()]\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {ignore_list_path}\")\n",
    "    ignore_list = []\n",
    "\n",
    "# Include non-UTF-8 encoding exclusion\n",
    "ignore_list.append('**/non-utf8-encoding.py')\n",
    "\n",
    "print(\"Ignore List:\", ignore_list)\n",
    "\n",
    "# Construct the path to the directory the loader will search\n",
    "load_directory = os.path.join(knowledgebase_dir, \"libs/core/langchain_core\")\n",
    "print(\"Directory to load files from:\", load_directory)\n",
    "\n",
    "\n",
    "# Configure the loader to handle all file types\n",
    "loader = GenericLoader.from_filesystem(\n",
    "    knowledgebase_dir,\n",
    "    glob=\"**/*\",\n",
    "    suffixes=[],\n",
    "    exclude=ignore_list,\n",
    "    parser=LanguageParser(language=Language.PYTHON, parser_threshold=500),\n",
    "    show_progress=True,\n",
    ")\n",
    "documents = loader.load()\n",
    "print(\"Documents Loaded:\", len(documents))\n",
    "for root, dirs, files in os.walk(knowledgebase_dir):\n",
    "    for name in files:\n",
    "        full_path = Path(root, name)\n",
    "        relative_path = full_path.relative_to(knowledgebase_dir)\n",
    "        if any(full_path.match(pattern) for pattern in ignore_list):\n",
    "            print(\"Ignored:\", f\"\\033[91m{relative_path}\\033[0m\")\n",
    "        else:\n",
    "            print(f\"\\033[92m{relative_path}\\033[0m\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split documents for processing\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, chunk_size=2000, chunk_overlap=200\n",
    ")\n",
    "texts = python_splitter.split_documents(documents)\n",
    "print(\"Texts processed:\", len(texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Setup Milvus\n",
    "from pymilvus import connections, CollectionSchema, FieldSchema, DataType, Collection\n",
    "connections.connect(\"default\", host=\"localhost\", port=\"19530\")\n",
    "fields = [\n",
    "    FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "    FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=768)  # Assuming embedding size of 768\n",
    "]\n",
    "schema = CollectionSchema(fields, description=\"Document embeddings\")\n",
    "collection = Collection(name=\"document_embeddings\", schema=schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate embeddings and insert into Milvus\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "db = Chroma.from_documents(texts, OpenAIEmbeddings(disallowed_special=()))\n",
    "embeddings = db.get_embeddings(texts)\n",
    "mr = collection.insert([embeddings])\n",
    "print(\"Embeddings inserted into Milvus:\", mr.primary_keys)\n",
    "\n",
    "# Setup retrieval from Milvus\n",
    "from pymilvus import SearchParams\n",
    "search_params = SearchParams(metric_type=\"L2\", params={\"nprobe\": 10})\n",
    "results = collection.search(embeddings[:1], \"embedding\", search_params, limit=10)\n",
    "print(\"Search results from Milvus:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Additional code for interacting with the embeddings\n",
    "from langchain.chains import create_retrieval_chain\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 8})\n",
    "document_chain = create_retrieval_chain(retriever)\n",
    "\n",
    "# Example retrieval\n",
    "question = \"What is the purpose of the Runnable class?\"\n",
    "response = document_chain.invoke(question)\n",
    "print(\"Retrieval response:\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
